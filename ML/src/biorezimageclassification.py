# -*- coding: utf-8 -*-
"""biorezImageClassification.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1-4fc0YwdlPbu7jte8KNx2WCMDhskmxnD

# Biorez Image Classification
"""

from google.colab import drive
drive.mount('/content/drive')

"""## Import"""

import tensorflow as tf
import numpy as np
import os
import matplotlib.pyplot as plt

from tensorflow.keras.applications import MobileNetV2
from tensorflow.keras.layers import Dense, GlobalAveragePooling2D
from tensorflow.keras.models import Model
from tensorflow.keras.optimizers import Adam
from sklearn.metrics import confusion_matrix, classification_report

"""## Load Data"""

tfrecord_path = '/content/drive/MyDrive/BIOREZv2/waste/dataset/train_data.tfrecord'

if not os.path.exists(tfrecord_path):
    raise FileNotFoundError(f"File tidak ditemukan: {tfrecord_path}")

IMG_HEIGHT = 224
IMG_WIDTH = 224
BATCH_SIZE = 8
NUM_CLASSES = 2
VALIDATION_SPLIT = 0.2

def _parse_image_function(example_proto):
    feature_description = {
        'image': tf.io.FixedLenFeature([], tf.string),
        'label': tf.io.FixedLenFeature([], tf.int64),
    }
    example = tf.io.parse_single_example(example_proto, feature_description)
    image = tf.image.decode_jpeg(example['image'], channels=3)
    image = tf.image.convert_image_dtype(image, tf.float32)
    image = tf.image.resize(image, [IMG_HEIGHT, IMG_WIDTH])
    label = tf.cast(example['label'], tf.int64)
    return image, label

def prepare_dataset(filepath, shuffle_buffer_size=1000):
    dataset = tf.data.TFRecordDataset(filepath)
    dataset = dataset.map(_parse_image_function, num_parallel_calls=tf.data.AUTOTUNE)
    return dataset

raw_dataset = prepare_dataset(tfrecord_path)
dataset_size = sum(1 for _ in raw_dataset)
print(f"Total data: {dataset_size}")

if dataset_size == 0:
    raise ValueError("Dataset kosong. Pastikan file TFRecord valid.")

val_size = int(dataset_size * VALIDATION_SPLIT)
train_size = dataset_size - val_size

train_ds = raw_dataset.take(train_size)
val_ds = raw_dataset.skip(train_size)

train_ds = train_ds.shuffle(1000).batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)
val_ds = val_ds.batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)

"""## Model Definition (MobileNetV2)"""

base_model = MobileNetV2(weights='imagenet', include_top=False, input_shape=(IMG_HEIGHT, IMG_WIDTH, 3))
x = base_model.output
x = GlobalAveragePooling2D()(x)
x = Dense(128, activation='relu')(x)
predictions = Dense(NUM_CLASSES, activation='softmax')(x)
model = Model(inputs=base_model.input, outputs=predictions)

for layer in base_model.layers:
    layer.trainable = False

model.compile(optimizer=Adam(), loss='sparse_categorical_crossentropy', metrics=['accuracy'])

initial_epochs = 10
history = model.fit(train_ds, epochs=initial_epochs, validation_data=val_ds)

# Visualisasikan hasil pelatihan awal
acc = history.history['accuracy']
val_acc = history.history['val_accuracy']
loss = history.history['loss']
val_loss = history.history['val_loss']

plt.figure(figsize=(10, 8))
plt.subplot(2, 1, 1)
plt.plot(acc, label='Training Accuracy')
plt.plot(val_acc, label='Validation Accuracy')
plt.plot([initial_epochs-1], [acc[-1]], 'o', color='blue', label=f'Final Train Acc: {acc[-1]:.2f}')
plt.plot([initial_epochs-1], [val_acc[-1]], 'o', color='orange', label=f'Final Val Acc: {val_acc[-1]:.2f}')
plt.legend(loc='lower right')
plt.title('Training and Validation Accuracy')

plt.subplot(2, 1, 2)
plt.plot(loss, label='Training Loss')
plt.plot(val_loss, label='Validation Loss')
plt.plot([initial_epochs-1], [loss[-1]], 'o', color='blue', label=f'Final Train Loss: {loss[-1]:.2f}')
plt.plot([initial_epochs-1], [val_loss[-1]], 'o', color='orange', label=f'Final Val Loss: {val_loss[-1]:.2f}')
plt.legend(loc='upper right')
plt.title('Training and Validation Loss')
plt.xlabel('epoch')
plt.show()

"""## Fine Tuning"""

base_model.trainable = True
fine_tune_at = 100

for layer in base_model.layers[:fine_tune_at]:
    layer.trainable = False

model.compile(optimizer=Adam(learning_rate=0.0001),
              loss=tf.keras.losses.SparseCategoricalCrossentropy(),
              metrics=['accuracy'])
model.summary()

fine_tune_epochs = 10
total_epochs = initial_epochs + fine_tune_epochs

history_fine = model.fit(train_ds,
                         epochs=total_epochs,
                         initial_epoch=history.epoch[-1],
                         validation_data=val_ds)

"""### Evaluate & Visualization"""

acc += history_fine.history['accuracy']
val_acc += history_fine.history['val_accuracy']
loss += history_fine.history['loss']
val_loss += history_fine.history['val_loss']

plt.figure(figsize=(10, 8))
plt.subplot(2, 1, 1)
plt.plot(acc, label='Training Accuracy')
plt.plot(val_acc, label='Validation Accuracy')
plt.plot([initial_epochs-1, total_epochs-1], [acc[initial_epochs-1], acc[-1]], 'o--', color='blue', label=f'Fine-tuned Train Acc: {acc[-1]:.2f}')
plt.plot([initial_epochs-1, total_epochs-1], [val_acc[initial_epochs-1], val_acc[-1]], 'o--', color='orange', label=f'Fine-tuned Val Acc: {val_acc[-1]:.2f}')
plt.legend(loc='lower right')
plt.title('Training and Validation Accuracy')

plt.subplot(2, 1, 2)
plt.plot(loss, label='Training Loss')
plt.plot(val_loss, label='Validation Loss')
plt.plot([initial_epochs-1, total_epochs-1], [loss[initial_epochs-1], loss[-1]], 'o--', color='blue', label=f'Fine-tuned Train Loss: {loss[-1]:.2f}')
plt.plot([initial_epochs-1, total_epochs-1], [val_loss[initial_epochs-1], val_loss[-1]], 'o--', color='orange', label=f'Fine-tuned Val Loss: {val_loss[-1]:.2f}')
plt.legend(loc='upper right')
plt.title('Training and Validation Loss')
plt.xlabel('epoch')
plt.show()

true_labels = []
predictions = []

for images, labels in val_ds:
    true_labels.extend(labels.numpy())

    preds = model.predict(images)

    predicted_classes = np.argmax(preds, axis=1)
    predictions.extend(predicted_classes)

true_labels = np.array(true_labels)
predictions = np.array(predictions)

print("\nConfusion Matrix:")
labels = ['Organik', 'Anorganik']
cm = confusion_matrix(true_labels, predictions)
print(cm)

print("\nClassification Report:")
target_names = ['Organik', 'Anorganik']
cr = classification_report(true_labels, predictions, target_names=target_names)
print(cr)

"""## Save Models"""

!pip install packaging==24.2

!pip install tensorflowjs

model.save("biorez.h5")

save_path = "saved_model/"
tf.saved_model.save(model, save_path)

!tensorflowjs_converter --input_format=tf_saved_model --output_format=tfjs_graph_model saved_model/ tfjs_model/

saved_model_dir = "/content/saved_model"

# Konversi langsung dari direktori SavedModel
converter = tf.lite.TFLiteConverter.from_saved_model(saved_model_dir)
converter.experimental_enable_resource_variables = True
converter.target_spec.supported_ops = [
    tf.lite.OpsSet.TFLITE_BUILTINS,
    tf.lite.OpsSet.SELECT_TF_OPS
]
converter.experimental_lower_tensor_list_ops = False

tflite_model = converter.convert()

# Simpan model TFLite
output_directory = "tflite"
os.makedirs(output_directory, exist_ok=True)
tflite_model_path = os.path.join(output_directory, "model.tflite")
with open(tflite_model_path, "wb") as f:
    f.write(tflite_model)

# Simpan label
labels = ['Organik', 'Anorganik']
labels_file_path = os.path.join(output_directory, "labels.txt")
with open(labels_file_path, "w") as f:
    for label in labels:
        f.write(label + "\n")